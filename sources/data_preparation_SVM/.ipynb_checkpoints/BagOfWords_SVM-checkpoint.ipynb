{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLabel(label):\n",
    "    if label =='ongoing-event':\n",
    "        return 0\n",
    "    elif label =='news':\n",
    "        return 1\n",
    "    elif label =='meme':\n",
    "        return 2\n",
    "    elif label =='commemorative':\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "count = [0,0,0,0]\n",
    "#read every trending and export json file\n",
    "with open('../../data/TT-annotations.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    trendingTopicArr = csv.reader(csvfile, delimiter=';')\n",
    "    for trendingTopic in trendingTopicArr:\n",
    "        path='../../../features/'+trendingTopic[0]+'.json'\n",
    "        data = json.load(open(path))\n",
    "        numberItem = len(data)\n",
    "        curLabel = toLabel(trendingTopic[3])\n",
    "        if count[curLabel] < 5:\n",
    "            count[curLabel]+=1\n",
    "            if (numberItem > 0):\n",
    "                for tweetJson in data:\n",
    "                    tweetJson = data[tweetJson]\n",
    "                    dataset.append([trendingTopic[3], tweetJson['tweet'].replace(\"RT \",\"\")])\n",
    "dataset = np.array(dataset)\n",
    "df = pd.DataFrame(data=dataset[0:,0:],    # values\n",
    "             columns=['label', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "commemorative    2353\n",
      "meme             1440\n",
      "news             1990\n",
      "ongoing-event    3388\n",
      "Name: tweets, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "col = ['label', 'tweet']\n",
    "df = df[col]\n",
    "df = df[pd.notnull(df['tweet'])]\n",
    "df.columns = ['label', 'tweets']\n",
    "df['label_id'] = df['label'].factorize()[0]\n",
    "label_id_df = df[['label', 'label_id']].drop_duplicates().sort_values('label_id')\n",
    "label_to_id = dict(label_id_df.values)\n",
    "id_to_category = dict(label_id_df[['label_id', 'label']].values)\n",
    "df.head()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "print(df.groupby('label').tweets.count())\n",
    "df.groupby('label').tweets.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'commemorative':\n",
      "  . Most correlated unigrams:\n",
      ". vanifacts\n",
      ". jesus\n",
      ". christ\n",
      ". bon\n",
      ". jovi\n",
      ". jon\n",
      ". mamonas\n",
      ". helau\n",
      ". welsh\n",
      ". killer\n",
      "# 'meme':\n",
      "  . Most correlated unigrams:\n",
      ". sad\n",
      ". applications\n",
      ". update\n",
      ". building\n",
      ". rachel\n",
      ". charliesheen\n",
      ". valley\n",
      ". sober\n",
      ". lodge\n",
      ". dealwithit\n",
      "# 'news':\n",
      "  . Most correlated unigrams:\n",
      ". director\n",
      ". corp\n",
      ". lse\n",
      ". davies\n",
      ". howard\n",
      ". sky\n",
      ". news\n",
      ". rip\n",
      ". russell\n",
      ". jane\n",
      "# 'ongoing-event':\n",
      "  . Most correlated unigrams:\n",
      ". goal\n",
      ". block\n",
      ". v√©lez\n",
      ". year\n",
      ". leap\n",
      ". sergeant\n",
      ". morkel\n",
      ". pepper\n",
      ". velez\n",
      ". mileyonsnl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(df.tweets).toarray()\n",
    "labels = df.label_id\n",
    "features.shape\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 10\n",
    "for label, label_id in sorted(label_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == label_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "#      bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  print(\"# '{}':\".format(label))\n",
    "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "#      print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df['tweets'], df['label'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(df['tweets'])\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X = tfidf_transformer.fit_transform(X_train_counts)\n",
    "y = df['tweets']\n",
    "#    clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "#    print('-------------',clf.predict(count_vect.transform([\"News to day: The world will destroy\"])))\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# classifier = SVC(kernel = 'linear', probability = True)\n",
    "# classifier.fit(X, df['label'])\n",
    "# import pickle\n",
    "# with open('SVM_BagofWords.pkl', 'wb') as fout:\n",
    "#     pickle.dump(classifier, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "models = [\n",
    "#    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    SVC(kernel = 'linear'),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, X, y, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df,\n",
    "              size=3, jitter=True, edgecolor=\"gray\", linewidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(cv_df.groupby('model_name').accuracy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "models = [\n",
    "#    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "    SGDClassifier(loss=\"hinge\", penalty=\"l2\"),\n",
    "#    MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\"),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, X, y, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df,\n",
    "              size=3, jitter=True, edgecolor=\"gray\", linewidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(cv_df.groupby('model_name').accuracy.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
